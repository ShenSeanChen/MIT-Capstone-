{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import itertools\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi linear approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(x, f, t):\n",
    "\n",
    "    sum_R = 0\n",
    "\n",
    "    for i in range(t):\n",
    "\n",
    "        x_bar = np.random.uniform(0,1, x.shape)\n",
    "\n",
    "        R_t = x >= x_bar\n",
    "\n",
    "        sum_R = sum_R + f(R_t)\n",
    "\n",
    "    return sum_R/t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to calculate gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_F(F,x,f,t,n):\n",
    "    \n",
    "    x_new_1 = x*np.ones((n, n))\n",
    "    x_new_0 = x*np.ones((n, n))\n",
    "\n",
    "    np.fill_diagonal(x_new_1, 1)\n",
    "    np.fill_diagonal(x_new_0, 0)\n",
    "    \n",
    "    return F(x_new_1, f, t) - F(x_new_0, f, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_ascent(F, x, f, alpha, t, max_iter, n):\n",
    "    \n",
    "    x_init = copy.deepcopy(x)\n",
    "    sum_init = F(x, f, t)\n",
    "    \n",
    "    # key values to be used\n",
    "    sum_update = 0\n",
    "    j = 0\n",
    "    \n",
    "    sum_temp = copy.deepcopy(sum_init)\n",
    "    # start updating the parameters x with iterative gradients\n",
    "    \n",
    "    while (j<max_iter):\n",
    "        \n",
    "        j += 1\n",
    "        sum_temp = F(x, f, t)\n",
    "        \n",
    "        grad = get_gradient_F(F,x,f,t,n)\n",
    "        \n",
    "        x = x + alpha*grad\n",
    "        x = np.maximum(np.minimum(x,1),0)\n",
    "        \n",
    "        sum_update = F(x, f, t)\n",
    "            \n",
    "        #print(\"Iteration: \", j, \"\\n\" , \"Function value: \", sum_temp,\"\\n\", \"x: \", x, \"\\n\")\n",
    "        #print(\"Iteration: \", j, \"\\n\" , \"Function value: \", sum_temp,\"\\n\", \"grad: \", grad, \"\\n\")\n",
    "        \n",
    "    \n",
    "    return j,sum_update, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def f_linear(x):\n",
    "        \n",
    "    return np.dot(a,x.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_polynomial(x):\n",
    "    d,n = x.shape\n",
    "    out = np.zeros((1,d))\n",
    "    \n",
    "    for i in range(0,d):\n",
    "        \n",
    "        x_i = x[i,:].reshape(1,n)\n",
    "        x_square = np.dot(x_i.T,x_i)\n",
    "        out[0,i] = np.sum(a_hat*x_square)\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the actual argmax by going through all 2^n alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actual_max(f,n):\n",
    "    \n",
    "    A = [np.array(i) for i in itertools.product([0, 1], repeat = n)]\n",
    "    \n",
    "    max_val_S = 0\n",
    "    argmax_s = []\n",
    "\n",
    "    for i in range(len(A)):\n",
    "\n",
    "        val_S = f(A[i].reshape(1,n))\n",
    "        \n",
    "        if val_S > max_val_S:\n",
    "            max_val_S = val_S\n",
    "            argmax_s = A[i]\n",
    "            print(i,max_val_S, argmax_s)\n",
    "    \n",
    "    return max_val_S, argmax_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for polynomial function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.13416285 0.86819873 0.68098077 0.08450305 0.80565954 0.85401028\n",
      "  0.6224054  0.70118783 0.53592099 0.54770508]]\n"
     ]
    }
   ],
   "source": [
    "#parameters\n",
    "alpha = 0.1 \n",
    "max_iter = 100\n",
    "t=500\n",
    "n = 10\n",
    "\n",
    "x_initial = np.random.uniform(0,1, (1,n))\n",
    "print(x_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        , -0.97979798, -0.95959596, -0.93939394, -0.91919192,\n",
       "        -0.8989899 , -0.87878788, -0.85858586, -0.83838384, -0.81818182],\n",
       "       [-0.7979798 , -0.77777778, -0.75757576, -0.73737374, -0.71717172,\n",
       "        -0.6969697 , -0.67676768, -0.65656566, -0.63636364, -0.61616162],\n",
       "       [-0.5959596 , -0.57575758, -0.55555556, -0.53535354, -0.51515152,\n",
       "        -0.49494949, -0.47474747, -0.45454545, -0.43434343, -0.41414141],\n",
       "       [-0.39393939, -0.37373737, -0.35353535, -0.33333333, -0.31313131,\n",
       "        -0.29292929, -0.27272727, -0.25252525, -0.23232323, -0.21212121],\n",
       "       [-0.19191919, -0.17171717, -0.15151515, -0.13131313, -0.11111111,\n",
       "        -0.09090909, -0.07070707, -0.05050505, -0.03030303, -0.01010101],\n",
       "       [ 0.01010101,  0.03030303,  0.05050505,  0.07070707,  0.09090909,\n",
       "         0.11111111,  0.13131313,  0.15151515,  0.17171717,  0.19191919],\n",
       "       [ 0.21212121,  0.23232323,  0.25252525,  0.27272727,  0.29292929,\n",
       "         0.31313131,  0.33333333,  0.35353535,  0.37373737,  0.39393939],\n",
       "       [ 0.41414141,  0.43434343,  0.45454545,  0.47474747,  0.49494949,\n",
       "         0.51515152,  0.53535354,  0.55555556,  0.57575758,  0.5959596 ],\n",
       "       [ 0.61616162,  0.63636364,  0.65656566,  0.67676768,  0.6969697 ,\n",
       "         0.71717172,  0.73737374,  0.75757576,  0.77777778,  0.7979798 ],\n",
       "       [ 0.81818182,  0.83838384,  0.85858586,  0.87878788,  0.8989899 ,\n",
       "         0.91919192,  0.93939394,  0.95959596,  0.97979798,  1.        ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_hat = np.linspace(-1, 1, n*n).reshape(n,n)\n",
    "a_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,\n",
       " array([[16.33333333]]),\n",
       " array([[0., 0., 0., 1., 1., 1., 1., 1., 1., 1.]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_ascent(F, x_initial, f_polynomial, alpha, t, max_iter, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [[1.]] [0 0 0 0 0 0 0 0 0 1]\n",
      "3 [[3.55555556]] [0 0 0 0 0 0 0 0 1 1]\n",
      "7 [[7.]] [0 0 0 0 0 0 0 1 1 1]\n",
      "15 [[10.66666667]] [0 0 0 0 0 0 1 1 1 1]\n",
      "31 [[13.88888889]] [0 0 0 0 0 1 1 1 1 1]\n",
      "63 [[16.]] [0 0 0 0 1 1 1 1 1 1]\n",
      "127 [[16.33333333]] [0 0 0 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[16.33333333]]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_max(f_polynomial,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for linear function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.         -0.77777778 -0.55555556 -0.33333333 -0.11111111  0.11111111\n",
      "   0.33333333  0.55555556  0.77777778  1.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, array([[2.77777778]]), array([[0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.linspace(-1, 1, n).reshape(1,n)\n",
    "print(a)\n",
    "gradient_ascent(F, x_initial, f_linear, alpha, t, max_iter, n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [[1.]] [0 0 0 0 0 0 0 0 0 1]\n",
      "3 [[1.77777778]] [0 0 0 0 0 0 0 0 1 1]\n",
      "7 [[2.33333333]] [0 0 0 0 0 0 0 1 1 1]\n",
      "15 [[2.66666667]] [0 0 0 0 0 0 1 1 1 1]\n",
      "31 [[2.77777778]] [0 0 0 0 0 1 1 1 1 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2.77777778]]), array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_max(f_linear,n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
